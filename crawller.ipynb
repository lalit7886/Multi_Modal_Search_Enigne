{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.wikipedia.org\"\n",
    "all_html= requests.get(base_url)\n",
    "print(\"Status code:\", all_html.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 0\n",
    "text_count = 0\n",
    "def extract_everything(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    base_url = url\n",
    "\n",
    "    # Create folders\n",
    "    folder_name_images = \"downloaded_images\"\n",
    "    folder_name_text = \"downloaded_texts\"\n",
    "    os.makedirs(folder_name_images, exist_ok=True)\n",
    "    os.makedirs(folder_name_text, exist_ok=True)\n",
    "\n",
    "\n",
    "    # --- Save images ---\n",
    "    for img_tag in soup.find_all('img', src=True):\n",
    "        img_src = img_tag['src']\n",
    "        img_url = urljoin(base_url, img_src)\n",
    "\n",
    "        try:\n",
    "            img_response = requests.get(img_url, timeout=10)\n",
    "            img = Image.open(BytesIO(img_response.content))\n",
    "            width, height = img.size\n",
    "\n",
    "            if width < 100 or height < 100:\n",
    "                print(f\"Skipped small image: {img_url} ({width}x{height})\")\n",
    "                continue\n",
    "\n",
    "            file_name = os.path.join(folder_name_images, f\"image_{image_count}.jpg\")\n",
    "            img.convert(\"RGB\").save(file_name)\n",
    "            print(f\"Downloaded: image_{image_count}.jpg ({width}x{height})\")\n",
    "            image_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download or process {img_url}: {e}\")\n",
    "\n",
    "    # --- Extract and save text ---\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()  # Remove script and style tags\n",
    "\n",
    "    text = soup.get_text(separator='\\n')\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    full_text = \"\\n\".join(lines)\n",
    "    file_name = os.path.join(folder_name_text, f\"text_{text_count}.txt\")\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "        print(f\"Text content saved to: {file_name}\")\n",
    "        text_count += 1\n",
    "        \n",
    "def extract_link(url):\n",
    "    response = requests.get(url)\n",
    "    content_html = BeautifulSoup(response.content, \"html.parser\")\n",
    "    base_url = url  # base for resolving relative links\n",
    "    links = []\n",
    "    \n",
    "    for a_tag in content_html.find_all('a', href=True):\n",
    "        full_url = urljoin(base_url, a_tag['href'])\n",
    "        links.append(full_url)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(start):\n",
    "    visited_nodes = set()\n",
    "    queue = deque([start])\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        \n",
    "        if node in visited_nodes:\n",
    "            continue\n",
    "        \n",
    "        visited_nodes.add(node)\n",
    "        \n",
    "        # Perform scraping\n",
    "        extract_everything(node)\n",
    "        \n",
    "        # Get and enqueue children\n",
    "        try:\n",
    "            child_nodes = extract_link(node)\n",
    "            for child in child_nodes:\n",
    "                if child not in visited_nodes:\n",
    "                    queue.append(child)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract links from {node}: {e}\")\n",
    "    \n",
    "    return visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download or process https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png: local variable 'image_count' referenced before assignment\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'text_count' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mbfs\u001b[0;34m(start)\u001b[0m\n\u001b[1;32m     11\u001b[0m visited_nodes\u001b[38;5;241m.\u001b[39madd(node)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform scraping\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mextract_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get and enqueue children\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mextract_everything\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     42\u001b[0m lines \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     43\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n\u001b[0;32m---> 44\u001b[0m file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_name_text, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(full_text)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'text_count' referenced before assignment"
     ]
    }
   ],
   "source": [
    "bfs(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
